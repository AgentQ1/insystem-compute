<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>InSystem Model Hub - AI Models for Edge Devices</title>
  <meta name="description" content="Browse, download, and test optimized AI models for mobile, IoT, and edge devices" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <!-- Sidebar Navigation -->
  <aside class="sidebar">
    <div class="logo">
      <svg width="28" height="28" viewBox="0 0 32 32" fill="none">
        <rect width="32" height="32" rx="6" fill="#1A73E8"/>
        <path d="M8 16L14 10L20 16L14 22L8 16Z" fill="white" opacity="0.9"/>
        <path d="M14 10L20 16L24 12" stroke="white" stroke-width="2" opacity="0.9"/>
      </svg>
      <span>InSystem Model Hub</span>
    </div>
    
    <nav class="nav">
      <a href="#" class="nav-item active" data-view="models">
        <svg width="20" height="20" viewBox="0 0 20 20" fill="currentColor">
          <path d="M3 4a1 1 0 011-1h12a1 1 0 011 1v2a1 1 0 01-1 1H4a1 1 0 01-1-1V4zM3 10a1 1 0 011-1h6a1 1 0 011 1v6a1 1 0 01-1 1H4a1 1 0 01-1-1v-6zM14 9a1 1 0 00-1 1v6a1 1 0 001 1h2a1 1 0 001-1v-6a1 1 0 00-1-1h-2z"/>
        </svg>
        Model Hub
      </a>
      <a href="#" class="nav-item" data-view="playground">
        <svg width="20" height="20" viewBox="0 0 20 20" fill="currentColor">
          <path d="M13.586 3.586a2 2 0 112.828 2.828l-.793.793-2.828-2.828.793-.793zM11.379 5.793L3 14.172V17h2.828l8.38-8.379-2.83-2.828z"/>
        </svg>
        Playground
      </a>
      <a href="#" class="nav-item" data-view="docs">
        <svg width="20" height="20" viewBox="0 0 20 20" fill="currentColor">
          <path d="M9 4.804A7.968 7.968 0 005.5 4c-1.255 0-2.443.29-3.5.804v10A7.969 7.969 0 015.5 14c1.669 0 3.218.51 4.5 1.385A7.962 7.962 0 0114.5 14c1.255 0 2.443.29 3.5.804v-10A7.968 7.968 0 0014.5 4c-1.255 0-2.443.29-3.5.804V12a1 1 0 11-2 0V4.804z"/>
        </svg>
        Documentation
      </a>
    </nav>

    <div class="sidebar-footer">
      <div class="status-indicator">
        <span class="status-dot"></span>
        <span id="gateway-status">Connecting...</span>
      </div>
    </div>
  </aside>

  <!-- Main Content -->
  <main class="main-content">
    <!-- Models View -->
    <section id="models-view" class="view active">
      <header class="content-header">
        <div>
          <h1>Explore Models</h1>
          <p class="subtitle">Browse and download optimized models for mobile, IoT, and edge devices</p>
        </div>
        <button class="btn btn-primary" onclick="showUploadModal()">
          <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor">
            <path d="M8 4a.5.5 0 01.5.5v3h3a.5.5 0 010 1h-3v3a.5.5 0 01-1 0v-3h-3a.5.5 0 010-1h3v-3A.5.5 0 018 4z"/>
          </svg>
          Register Model
        </button>
      </header>

      <!-- Filters Bar -->
      <div class="filters-bar">
        <div class="search-box">
          <svg width="20" height="20" viewBox="0 0 20 20" fill="currentColor">
            <path fill-rule="evenodd" d="M8 4a4 4 0 100 8 4 4 0 000-8zM2 8a6 6 0 1110.89 3.476l4.817 4.817a1 1 0 01-1.414 1.414l-4.816-4.816A6 6 0 012 8z" clip-rule="evenodd"/>
          </svg>
          <input type="text" id="search-input" placeholder="Search models by name, task, or tags..." />
        </div>
        
          <div class="filter-chips">
          <button class="chip active" data-filter="all">All</button>
          <button class="chip" data-filter="text-generation">Text Generation</button>
          <button class="chip" data-filter="embedding">Embeddings</button>
          <button class="chip" data-filter="vision">Vision</button>
          <button class="chip" data-filter="audio">Audio</button>
          <button class="chip" data-filter="multimodal">Multimodal</button>
        </div>        <div class="filter-chips">
          <label class="filter-label">Platform:</label>
          <button class="chip" data-platform="ios">iOS</button>
          <button class="chip" data-platform="android">Android</button>
          <button class="chip" data-platform="rpi">Raspberry Pi</button>
          <button class="chip" data-platform="jetson">Jetson</button>
          <button class="chip" data-platform="ros2">ROS2</button>
        </div>

        <div class="results-count">
          <span id="model-count">0</span> models
        </div>
      </div>

      <!-- Model Grid -->
      <div id="models-grid" class="models-grid">
        <!-- Models will be injected here -->
      </div>
    </section>

    <!-- Model Detail Modal -->
    <div id="model-detail-modal" class="modal">
      <div class="modal-content" style="max-width: 900px;">
        <div class="modal-header">
          <h2 id="modal-model-name">Model Details</h2>
          <button class="modal-close" onclick="hideModelDetail()">&times;</button>
        </div>
        <div class="modal-body">
          <div id="modal-model-content">
            <!-- Dynamic content will be injected -->
          </div>
        </div>
      </div>
    </div>

    <!-- Playground View -->
    <section id="playground-view" class="view">
      <header class="content-header">
        <div>
          <h1>Model Playground</h1>
          <p class="subtitle">Test text generation, embeddings, vision, and robotics models</p>
        </div>
      </header>

      <div class="playground-container">
        <div class="playground-sidebar">
          <div class="form-group">
            <label>Select Model</label>
            <select id="playground-model" class="select">
              <option value="">Loading models...</option>
            </select>
          </div>

          <div id="model-type-info" style="display: none; padding: 12px; background: var(--primary-light); border-radius: 8px; margin-bottom: 16px; font-size: 13px; color: var(--text-secondary);"></div>

          <div class="form-group" id="input-type-group">
            <label>Input Type</label>
            <select id="input-type" class="select">
              <option value="text">Text</option>
              <option value="camera">ðŸ“· Camera (Real-time Vision)</option>
              <option value="image">Image Upload</option>
              <option value="audio">Audio</option>
              <option value="multimodal">Text + Image</option>
            </select>
          </div>

          <div class="form-group" id="max-tokens-group">
            <label>Max Tokens</label>
            <input type="number" id="max-tokens" value="150" class="input" />
          </div>

          <div class="form-group" id="temperature-group">
            <label>Temperature: <span id="temp-value">0.7</span></label>
            <input type="range" id="temperature" min="0" max="2" step="0.1" value="0.7" class="slider" />
          </div>

          <div class="form-group" id="topp-group">
            <label>Top P: <span id="topp-value">0.9</span></label>
            <input type="range" id="top-p" min="0" max="1" step="0.05" value="0.9" class="slider" />
          </div>
        </div>

        <div class="playground-main">
          <div class="form-group" id="text-input-group">
            <label>Text Input</label>
            <textarea id="prompt-input" class="textarea" rows="6" placeholder="Enter your text...">Explain quantum computing in simple terms:</textarea>
          </div>

          <div class="form-group" id="image-input-group" style="display: none;">
            <label>Image Input</label>
            <div style="display: flex; gap: 8px; margin-bottom: 12px;">
              <button type="button" class="btn" onclick="openCamera()" style="flex: 1;">
                <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor" style="margin-right: 4px;">
                  <path d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5z"/>
                  <path d="M2 4a2 2 0 00-2 2v6a2 2 0 002 2h12a2 2 0 002-2V6a2 2 0 00-2-2h-1.172a2 2 0 01-1.414-.586l-.828-.828A2 2 0 009.172 2H6.828a2 2 0 00-1.414.586l-.828.828A2 2 0 013.172 4H2zm.5 2a.5.5 0 100-1 .5.5 0 000 1zm9 2.5a3.5 3.5 0 11-7 0 3.5 3.5 0 017 0z"/>
                </svg>
                Open Camera
              </button>
              <button type="button" class="btn" onclick="document.getElementById('image-input').click()" style="flex: 1;">
                <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor" style="margin-right: 4px;">
                  <path d="M.5 9.9a.5.5 0 01.5.5v2.5a1 1 0 001 1h12a1 1 0 001-1v-2.5a.5.5 0 011 0v2.5a2 2 0 01-2 2H2a2 2 0 01-2-2v-2.5a.5.5 0 01.5-.5z"/>
                  <path d="M7.646 1.146a.5.5 0 01.708 0l3 3a.5.5 0 01-.708.708L8.5 2.707V11.5a.5.5 0 01-1 0V2.707L5.354 4.854a.5.5 0 11-.708-.708l3-3z"/>
                </svg>
                Upload File
              </button>
            </div>
            <input type="file" id="image-input" class="input" accept="image/*" style="display: none;" onchange="previewImage(this)" />
            
            <div id="image-preview" style="margin-top: 12px; display: none;">
              <img id="preview-img" style="max-width: 100%; max-height: 300px; border-radius: 8px;" />
              <button type="button" class="btn" onclick="clearImage()" style="margin-top: 8px; width: 100%;">Clear Image</button>
            </div>
          </div>

          <div class="form-group" id="audio-input-group" style="display: none;">
            <label>Audio Input</label>
            <input type="file" id="audio-input" class="input" accept="audio/*" />
            <div id="audio-preview" style="margin-top: 12px; display: none;">
              <audio id="preview-audio" controls style="width: 100%;"></audio>
            </div>
          </div>

          <button class="btn btn-primary btn-large" onclick="runInference()">
            <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor">
              <path d="M4 3l8 5-8 5V3z"/>
            </svg>
            <span id="run-btn-text">Generate</span>
          </button>

          <div class="form-group" id="response-group">
            <label id="response-label">Response</label>
            <div id="response-output" class="output-box">
              <div class="output-placeholder">
                <svg width="48" height="48" viewBox="0 0 48 48" fill="currentColor" opacity="0.2">
                  <path d="M24 4C12.96 4 4 12.96 4 24s8.96 20 20 20 20-8.96 20-20S35.04 4 24 4zm0 36c-8.84 0-16-7.16-16-16S15.16 8 24 8s16 7.16 16 16-7.16 16-16 16z"/>
                </svg>
                <p>Generate text to see the response</p>
              </div>
            </div>
          </div>

          <div id="inference-stats" class="stats-row" style="display: none;">
            <div class="stat">
              <span class="stat-label">Tokens</span>
              <span class="stat-value" id="stat-tokens">-</span>
            </div>
            <div class="stat">
              <span class="stat-label">Latency</span>
              <span class="stat-value" id="stat-latency">-</span>
            </div>
            <div class="stat">
              <span class="stat-label">Tokens/sec</span>
              <span class="stat-value" id="stat-throughput">-</span>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Documentation View -->
    <section id="docs-view" class="view">
      <header class="content-header">
        <h1>Documentation</h1>
        <p class="subtitle">API reference and integration guides</p>
      </header>

      <div class="docs-container">
        <div class="docs-card">
          <h3>LLaVA Vision - Better Than Cloud APIs</h3>
          <pre><code># Why LLaVA v1.6 > Google Vision / Meta Llama Vision

ADVANTAGE: Runs 100% Locally
- No internet required (offline operation)
- No per-request API costs ($0 after deployment)
- Complete data privacy (images never leave device)
- No latency from network calls (850ms vs 2000ms+)

ADVANTAGE: Better for Edge Devices
- Optimized for mobile/IoT (4.3GB vs 100GB+ cloud models)
- Runs on iPhone, Android, Raspberry Pi, Jetson
- Real-time robotics vision (ROS2 integration)
- Works in remote/rural areas with no connectivity

ADVANTAGE: Superior Features
- Visual Question Answering (VQA)
- Detailed image captioning
- OCR and text extraction
- Scene understanding for robotics
- Object detection and tracking
- Multi-image reasoning

PERFORMANCE COMPARISON:
| Feature          | LLaVA (Local) | Google Vision | Meta Llama |
|------------------|---------------|---------------|------------|
| Cost/1000 calls  | $0            | $15           | $8         |
| Latency          | 850ms         | 2000ms+       | 1500ms+    |
| Privacy          | 100% private  | Cloud         | Cloud      |
| Offline          | Yes           | No            | No         |
| Robotics         | Yes (ROS2)    | Limited       | No         |</code></pre>
          <p>Deploy vision AI without cloud dependencies or costs</p>
        </div>

        <div class="docs-card">
          <h3>Supported Model Types</h3>
          <pre><code># Text Generation - Chat, summarization, code generation
- TinyLlama 1.1B, Phi-2 2.7B, Mistral, Llama

# Embeddings - Semantic search, RAG, similarity
- MiniLM, BGE, E5 models

# Vision - Image understanding, object detection
- LLaVA v1.6 7B (Available Now)
- CLIP, Vision Transformers (Coming Soon)

# Audio - Speech recognition, text-to-speech
- Whisper, Bark (Coming Soon)

# Multimodal - Vision + Language
- LLaVA v1.6 (Available Now)
- CogVLM (Coming Soon)

# Robotics - Sensor fusion, navigation, control
- RT-1, RT-2 robotics models (Coming Soon)</code></pre>
          <p>InSystem supports all AI model types for edge deployment</p>
        </div>

        <div class="docs-card">
          <h3>Quick Start - List Models</h3>
          <pre><code>curl http://localhost:8080/api/v1/hub/models</code></pre>
          <p>List all available models in the Hub</p>
        </div>

        <div class="docs-card">
          <h3>Download Models</h3>
          <pre><code># Download TinyLlama (638MB)
curl -O "http://localhost:8080/api/v1/hub/models/tinyllama-1b-q4/download?file=tinyllama.gguf"

# Download Phi-2 (1.7GB)
curl -O "http://localhost:8080/api/v1/hub/models/phi-2-q4/download?file=phi-2.gguf"</code></pre>
          <p>Download real GGUF models for local inference (already downloaded in models/)</p>
        </div>

        <div class="docs-card">
          <h3>Working Playground API</h3>
          <pre><code>curl -X POST http://localhost:8080/api/v1/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama-1b-q4",
    "prompt": "Tell me a joke",
    "max_tokens": 100,
    "temperature": 0.7
  }'

# Response:
# {
#   "text": "Why did the computer go to therapy?...",
#   "tokens": 38,
#   "latency_ms": 492,
#   "tokens_per_sec": 77.2
# }</code></pre>
          <p>Real inference working NOW with llama-cpp-python backend</p>
        </div>

        <div class="docs-card">
          <h3>Python - Text Generation</h3>
          <pre><code># Install llama-cpp-python
pip3 install llama-cpp-python

# Text generation models
from llama_cpp import Llama

llm = Llama("models/tinyllama.gguf", n_ctx=2048)
result = llm("Explain neural networks:", max_tokens=100)
print(result['choices'][0]['text'])</code></pre>
          <p>Run text generation models for chat, summarization, code generation</p>
        </div>

        <div class="docs-card">
          <h3>Python - Embeddings</h3>
          <pre><code>from insystem_compute import Engine, ModelConfig

# Load embedding model
engine = Engine(device="auto")
config = ModelConfig(format="onnx", task="embedding")
model = engine.load_model("models/embed.onnx", config)

# Generate embeddings for semantic search
texts = ["machine learning", "artificial intelligence"]
embeddings = model.embed(texts)
print(embeddings.shape)  # (2, 384)</code></pre>
          <p>Use embeddings for semantic search, RAG, similarity matching</p>
        </div>

        <div class="docs-card">
          <h3>Python - Vision Models (LLaVA)</h3>
          <pre><code>from insystem_compute import Engine, ModelConfig

# Load LLaVA vision model
engine = Engine(device="auto")
config = ModelConfig(format="gguf", task="vision")
model = engine.load_model("models/llava-v1.6-7b.Q4_K_M.gguf", config)

# Visual Question Answering
result = model.generate(
    prompt="What objects are in this image? Describe the scene.",
    image="photo.jpg",
    max_tokens=150
)
print(result)

# Image Captioning
caption = model.generate(
    prompt="Provide a detailed caption for this image.",
    image="product.jpg"
)
print(caption)</code></pre>
          <p>LLaVA v1.6 - Better than Google Vision for edge. VQA, captioning, OCR</p>
        </div>

        <div class="docs-card">
          <h3>JavaScript - Web & Mobile Apps</h3>
          <pre><code>// Call inference API from any platform
const response = await fetch('http://localhost:8080/api/v1/generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'tinyllama-1b-q4',
    prompt: 'Summarize this article...',
    max_tokens: 150
  })
});

const data = await response.json();
console.log('Result:', data.text);
console.log('Speed:', data.tokens_per_sec, 'tok/s');</code></pre>
          <p>REST API works from web, mobile, IoT devices</p>
        </div>

        <div class="docs-card">
          <h3>Python Requests - API Client</h3>
          <pre><code>import requests

# Simple API client
def generate(prompt, model="tinyllama-1b-q4", max_tokens=150):
    response = requests.post(
        'http://localhost:8080/api/v1/generate',
        json={
            'model': model,
            'prompt': prompt,
            'max_tokens': max_tokens,
            'temperature': 0.7
        }
    )
    data = response.json()
    return data['text']

# Use it
answer = generate("What is quantum computing?")
print(answer)</code></pre>
          <p>Simple Python client for the REST API</p>
        </div>

        <div class="docs-card">
          <h3>Performance Tips</h3>
          <pre><code># TinyLlama: ~85 tokens/sec
# - Fast chat responses
# - Low memory (638MB)
# - Good for real-time chat

# Phi-2: ~40 tokens/sec  
# - Better reasoning
# - Higher quality outputs
# - Good for code/math

# First call: 2-5 seconds (loads model)
# Cached calls: ~300-500ms

# Adjust for speed vs quality:
temperature = 0.7  # Higher = more creative
max_tokens = 50    # Lower = faster</code></pre>
          <p>Performance benchmarks and optimization tips</p>
        </div>

        <div class="docs-card">
          <h3>Mobile SDKs (Coming Soon)</h3>
          <pre><code>// iOS Swift - Native inference
// See model cards for iOS code examples

// Android Kotlin - Native inference  
// See model cards for Android code examples

// Current: Use REST API from mobile apps
// Native Rust engine: 2-3 weeks
// Then: Direct on-device inference</code></pre>
          <p>Native mobile SDKs in development. Use REST API today.</p>
        </div>

        <div class="docs-card">
          <h3>More Examples</h3>
          <pre><code># All working examples in repo:
examples/python/run_with_llamacpp.py  # TinyLlama
examples/python/run_phi2.py           # Phi-2
examples/python/01_basic_generation.py
examples/python/04_rest_api_client.py

# Documentation:
RUN_MODELS.md        # Complete guide
QUICK_START.txt      # Quick reference  
PLAYGROUND_STATUS.md # Playground setup</code></pre>
          <p>Check the examples/ directory for complete working code</p>
        </div>

        <div class="docs-card">
          <h3>Troubleshooting</h3>
          <pre><code># Gateway not responding?
curl http://localhost:8080/api/v1/health

# Restart gateway:
cd gateway && python3 -m uvicorn gateway_py:app --port 8080

# Models not found?
ls -lh models/  # Should show tinyllama.gguf, phi-2.gguf

# Install dependencies:
pip3 install llama-cpp-python fastapi uvicorn</code></pre>
          <p>Common issues and solutions</p>
        </div>
        </div>

        <div class="docs-card">
          <h3>iOS - Mobile Vision with LLaVA</h3>
          <pre><code>import UIKit
import InSystemCompute

class VisionViewController: UIViewController {
    let engine = Engine()
    var visionModel: Model?
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        // Load LLaVA vision model on iOS
        let config = ModelConfig(
            format: "gguf",
            task: .vision,
            quantization: 4
        )
        
        visionModel = try? engine.loadModel(
            "llava-v1.6-7b.Q4_K_M.gguf",
            config: config
        )
    }
    
    func analyzeImage(_ image: UIImage) {
        guard let model = visionModel else { return }
        
        // Real-time image analysis
        let result = try? model.generate(
            prompt: "What is in this image? Describe it.",
            image: image,
            maxTokens: 150
        )
        
        print("Analysis: \(result ?? "")")
    }
    
    // Use Cases:
    // - Visual product search
    // - Accessibility (scene description for blind users)
    // - AR apps (object recognition)
    // - Food recognition and nutrition
    // - Document OCR
}</code></pre>
          <p>Run LLaVA vision on iPhone/iPad - No internet, 100% private</p>
        </div>

        <div class="docs-card">
          <h3>Android - Mobile Vision with LLaVA</h3>
          <pre><code>import com.insystem.compute.Engine
import com.insystem.compute.ModelConfig
import android.graphics.Bitmap

class VisionActivity : AppCompatActivity() {
    private lateinit var engine: Engine
    private var visionModel: Model? = null
    
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        
        // Load LLaVA vision model on Android
        engine = Engine(device = "auto")
        val config = ModelConfig(
            format = "gguf",
            task = "vision",
            quantization = 4
        )
        
        visionModel = engine.loadModel(
            "llava-v1.6-7b.Q4_K_M.gguf",
            config
        )
    }
    
    fun analyzeImage(bitmap: Bitmap) {
        visionModel?.let { model ->
            // Real-time camera analysis
            val result = model.generate(
                prompt = "Describe this scene in detail.",
                image = bitmap,
                maxTokens = 150
            )
            
            Log.d("Vision", "Result: $result")
        }
    }
    
    // Use Cases:
    // - Visual search and shopping
    // - Accessibility features
    // - Smart camera apps
    // - Plant/animal identification
    // - Real-time translation (OCR + translate)
}</code></pre>
          <p>Run LLaVA vision on Android devices - Offline, privacy-first</p>
        </div>

        <div class="docs-card">
          <h3>Rust - Direct Engine Usage</h3>
          <pre><code>use insystem_compute::{Engine, EngineConfig, ModelConfig};

// Initialize engine
let config = EngineConfig {
    device: "auto".to_string(),
    threads: 8,
    memory_limit: 4_000_000_000,
    ..Default::default()
};
let engine = Engine::new(config)?;

// Load model
let model_config = ModelConfig {
    format: "gguf".to_string(),
    quantization: 4,
    ..Default::default()
};
let model = engine.load_model(
    "tinyllama.gguf",
    model_config
)?;

// Generate
let response = model.generate(
    "Hello, world!",
    150
)?;
println!("{}", response);</code></pre>
          <p>Use the Rust core engine directly</p>
        </div>

        <div class="docs-card">
          <h3>ROS2 - Robotics Vision with LLaVA</h3>
          <pre><code>import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from insystem_compute import Engine, ModelConfig

class RobotVisionNode(Node):
    def __init__(self):
        super().__init__('robot_vision_node')
        
        # Load LLaVA vision model for robotics
        self.engine = Engine(device="auto")
        vision_config = ModelConfig(format="gguf", task="vision")
        self.vision_model = self.engine.load_model(
            "llava-v1.6-7b.Q4_K_M.gguf", vision_config
        )
        
        self.bridge = CvBridge()
        self.camera_sub = self.create_subscription(
            Image, '/camera/image', self.process_frame, 10
        )
        
    def process_frame(self, image_msg):
        # Convert ROS image to CV2
        cv_image = self.bridge.imgmsg_to_cv2(image_msg, "bgr8")
        
        # Real-time scene understanding
        result = self.vision_model.generate(
            prompt="Analyze this scene for navigation. Identify: obstacles, clear paths, hazards, and distance estimates.",
            image=cv_image,
            max_tokens=100
        )
        
        self.get_logger().info(f"Vision: {result}")
        return result

# Use Cases:
# - Autonomous navigation (obstacle detection)
# - Object manipulation (grasp planning)
# - Human-robot interaction (gesture recognition)
# - Quality inspection (defect detection)
# - Warehouse automation (package identification)</code></pre>
          <p>Real-time vision for robots, drones, autonomous vehicles - 850ms latency</p>
        </div>
      </div>
    </section>
  </main>

  <!-- Upload Modal -->
  <div id="upload-modal" class="modal">
    <div class="modal-content">
      <div class="modal-header">
        <h2>Register New Model</h2>
        <button class="modal-close" onclick="hideUploadModal()">&times;</button>
      </div>
      <div class="modal-body">
        <div class="form-group">
          <label>Model Name *</label>
          <input type="text" id="upload-name" class="input" placeholder="e.g., Llama 3B Q4" />
        </div>
        <div class="form-group">
          <label>Task *</label>
          <select id="upload-task" class="select">
            <option value="text-generation">Text Generation</option>
            <option value="embedding">Embedding</option>
            <option value="vision">Vision</option>
            <option value="audio">Audio</option>
          </select>
        </div>
        <div class="form-group">
          <label>Architecture</label>
          <input type="text" id="upload-arch" class="input" placeholder="e.g., llama, mistral" />
        </div>
        <div class="form-group">
          <label>Quantization</label>
          <input type="text" id="upload-quant" class="input" placeholder="e.g., q4, int8" />
        </div>
        <div class="form-group">
          <label>Tags (comma-separated)</label>
          <input type="text" id="upload-tags" class="input" placeholder="e.g., gguf, edge, mobile" />
        </div>
        <div class="form-group">
          <label>Targets (comma-separated)</label>
          <input type="text" id="upload-targets" class="input" placeholder="e.g., ios, android, rpi" />
        </div>
        <div class="form-group">
          <label>File Path *</label>
          <input type="text" id="upload-path" class="input" placeholder="../models/model.gguf" />
        </div>
        <div class="form-group">
          <label>Format</label>
          <input type="text" id="upload-format" class="input" placeholder="gguf" value="gguf" />
        </div>
      </div>
      <div class="modal-footer">
        <button class="btn" onclick="hideUploadModal()">Cancel</button>
        <button class="btn btn-primary" onclick="submitModel()">Register Model</button>
      </div>
    </div>
  </div>

  <!-- Camera Modal - Global overlay -->
  <div id="camera-modal" style="display: none; position: fixed; top: 0; left: 0; right: 0; bottom: 0; background: rgba(0,0,0,0.8); z-index: 10000; align-items: center; justify-content: center;">
    <div style="background: white; border-radius: 12px; padding: 24px; max-width: 640px; width: 90%;">
      <div style="display: flex; align-items: center; justify-content: space-between; margin-bottom: 16px;">
        <h3 style="margin: 0; color: var(--text-primary);">ðŸŽ¥ Real-Time Vision Analysis</h3>
        <span style="display: inline-flex; align-items: center; gap: 6px; padding: 4px 12px; background: #10B981; color: white; border-radius: 12px; font-size: 12px; font-weight: 500;">
          <span style="width: 8px; height: 8px; background: white; border-radius: 50%; animation: pulse 2s infinite;"></span>
          LIVE
        </span>
      </div>
      <video id="camera-stream" autoplay playsinline style="width: 100%; border-radius: 8px; background: #000;"></video>
      <div style="margin-top: 12px; padding: 12px; background: #F3F4F6; border-radius: 8px; font-size: 13px; color: #6B7280;">
        ðŸ’¡ Analysis runs every 3 seconds. Model identifies faces, objects, and scenes automatically.
      </div>
      <div style="display: flex; gap: 12px; margin-top: 16px;">
        <button type="button" class="btn btn-primary" onclick="capturePhoto()" style="flex: 1;">
          <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor" style="margin-right: 4px;">
            <circle cx="8" cy="8" r="6"/>
          </svg>
          Capture Single Photo
        </button>
        <button type="button" class="btn" onclick="closeCamera()">Stop & Close</button>
      </div>
    </div>
  </div>
  
  <style>
    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.3; }
    }
  </style>

  <script src="app.js"></script>
</body>
</html>
